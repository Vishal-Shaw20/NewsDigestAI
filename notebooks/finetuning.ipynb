{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T20:20:10.607615Z",
     "start_time": "2025-10-28T20:20:10.598271Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.optimizers import schedules, AdamW\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:20:35.578538Z",
     "start_time": "2025-10-28T20:20:11.768175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = load_dataset('cnn_dailymail','3.0.0',split='train').shuffle(seed=42).select(range(1000))\n",
    "val_dataset = load_dataset('cnn_dailymail','3.0.0',split='validation').shuffle(seed=42).select(range(100))"
   ],
   "id": "d45454b5625b466e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:20:44.607341Z",
     "start_time": "2025-10-28T20:20:44.590889Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset.features",
   "id": "647d129d24604570",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': Value(dtype='string', id=None),\n",
       " 'highlights': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:20:45.689262Z",
     "start_time": "2025-10-28T20:20:45.283538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_checkpoint = 't5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "id": "7e7b055c5c738226",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:20:46.975782Z",
     "start_time": "2025-10-28T20:20:46.962367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prefix = 'summarize: '\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(sample):\n",
    "    inputs = [prefix + t for t in sample['article']]\n",
    "    model_inputs = tokenizer(inputs,\n",
    "                             max_length=max_input_length,\n",
    "                             truncation=True,\n",
    "                             padding='max_length')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(sample['highlights'],\n",
    "                          max_length=max_target_length,\n",
    "                          truncation=True,\n",
    "                          padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ],
   "id": "bebb73b15a0f307c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:20:48.188409Z",
     "start_time": "2025-10-28T20:20:48.142879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ],
   "id": "1d5a94c16b704680",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:20:49.347373Z",
     "start_time": "2025-10-28T20:20:49.329787Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_train_dataset",
   "id": "203711735d17ce03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:20:51.431714Z",
     "start_time": "2025-10-28T20:20:50.943050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "tf_dataset_columns = ['input_ids','attention_mask','labels']\n",
    "tf_train_dataset = tokenized_train_dataset.to_tf_dataset(\n",
    "    columns=tf_dataset_columns,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "tf_val_dataset = tokenized_val_dataset.to_tf_dataset(\n",
    "    columns=tf_dataset_columns,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "id": "8508dfb179b15d7d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:21:28.019224Z",
     "start_time": "2025-10-28T20:21:25.758860Z"
    }
   },
   "cell_type": "code",
   "source": "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)",
   "id": "b0ffd01d96b50799",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T20:21:31.604578Z",
     "start_time": "2025-10-28T20:21:31.574765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "initial_learning_rate = 2e-5\n",
    "end_learning_rate = 0.0\n",
    "lr_schedule = schedules.PolynomialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    end_learning_rate=end_learning_rate,\n",
    "    decay_steps=num_train_steps\n",
    ")\n",
    "optimizer = AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "model.optimizer = optimizer\n",
    "model._is_compiled=True\n",
    "print('Model Compiled Successfully!!')"
   ],
   "id": "7840b822cc518cff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compiled Successfully!!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "print(\"Starting manual training loop...\")\n",
    "num_epochs=3\n",
    "num_train_steps_per_epoch = len(tf_train_dataset)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n--- Starting Epoch {epoch + 1}/{num_epochs} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Training ---\n",
    "    for step, batch in enumerate(tf_train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 1. Forward pass: Get model predictions and loss\n",
    "            outputs = model(batch, training=True)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # 2. Calculate gradients\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        # 3. Apply gradients (update weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if (step + 1) % 20 == 0: # Print a log every 20 steps\n",
    "            print(f\"  Step {step + 1}/{num_train_steps_per_epoch}, Loss: {tf.reduce_mean(loss).numpy():.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    print(\"\\nRunning validation...\")\n",
    "    total_val_loss = 0\n",
    "    num_val_steps = 0\n",
    "\n",
    "    for batch in tf_val_dataset:\n",
    "        # Run in inference mode (no gradients)\n",
    "        outputs = model(batch, training=False)\n",
    "        total_val_loss += outputs.loss.numpy()\n",
    "        num_val_steps += 1\n",
    "\n",
    "    avg_val_loss = total_val_loss / num_val_steps\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    print(f\"--- Epoch {epoch + 1} Summary ---\")\n",
    "    print(f\"Time: {epoch_time:.2f}s, Validation Loss: {avg_val_loss.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ],
   "id": "4c5e67fff439929a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
